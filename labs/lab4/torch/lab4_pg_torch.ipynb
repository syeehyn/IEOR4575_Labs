{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"lab4\", entity=\"ieor-4575\", tags=[\"torch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "In the previous lab, we talked about value based method for reinforcement learning. In this lab, we focus on policy based method.\n",
    "\n",
    "In policy based methods, intead of defining a value function $Q_\\theta(s,a)$ and inducing a policy based on argmax, we parameterize a stochastic policy directly. The policy is parameterized as a categorical distribution over actions. Let it be $\\pi_\\phi(s)$ with parameter $\\phi$, then the policy is defined by sampling actions $$a \\sim \\pi_\\phi(s)$$\n",
    "\n",
    "The policy induces a probability $p(\\tau)$ over trajectories $\\tau = \\{s_0,a_0,s_1,a_1,..\\}$. The expected total discounted reward is \n",
    "\n",
    "$$\\rho(\\phi) = \\mathbb{E}_{\\tau \\sim p(\\tau)} \\big[R(\\tau)\\big] = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty r_t \\gamma^t \\big]$$\n",
    "\n",
    "The aim is to find $\\phi$ such that the expected reward induced by $\\pi_\\phi$ is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Computation\n",
    "\n",
    "We can derive policy gradient\n",
    "\n",
    "$$\\nabla_\\phi \\rho(\\phi) = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty Q^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
    "\n",
    "To compute the gradient for update $\\phi \\leftarrow \\phi + \\alpha \\nabla_\\phi \\rho(\\phi)$, we need to estimate $Q^{\\pi_\\phi}(s,a)$. Since $Q^{\\pi_\\phi}(s,a)$ is usually not analytically accessible, it can be approximated by \n",
    "1. Monte Carlo estimate\n",
    "2. Train a value function $Q_\\theta(s,a) \\approx Q^{\\pi_\\phi}(s,a)$ and use it as a proxy\n",
    "3. Mixture of both above\n",
    "\n",
    "Before estimating $Q^{\\pi_\\phi}(s,a)$, let us write a parameterized policy over actions. The policy $\\pi_\\phi(s)$ takes a state as input and outputs a categorical distribution over actions. For example, if we have two actions, the probability vector to output is of the form $\\pi_\\phi(s)=[0.6,0.4]$.\n",
    "\n",
    "**Loss function**\n",
    "Given samples of state action pairs $(s_i, a_i)$ and estimate $Q_i$ for $Q^{\\pi_\\phi}(s_i, a_i)$, for $i=1,\\ldots, $ we  set the loss function  as \n",
    "$$-\\frac{1}{N} \\sum_{i=1}^N Q_i \\log(\\pi_\\phi(s_i, a_i)) $$\n",
    "The loss function enables us  to compute policy gradients in implementation. The negative gradient of the above loss function has the form\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N Q_i  \\nabla_\\phi \\log \\pi_\\phi(s_i, a_i) $$\n",
    "\n",
    "where $Q_i$s are estimated and $\\nabla_\\phi \\log\\pi_\\phi(s_i, a_i)$s are computed via backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net \\pi_\\phi(s) as a class\n",
    "\n",
    "class Policy(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, lr):\n",
    "        \"\"\"\n",
    "        obssize: size of the states\n",
    "        actsize: size of the actions\n",
    "        \"\"\"\n",
    "        # TODO DEFINE THE MODEL\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    #input layer of input size obssize\n",
    "                    #intermediate layers\n",
    "                    #output layer of output size actsize\n",
    "                )\n",
    "        \n",
    "        # DEFINE THE OPTIMIZER\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # RECORD HYPER-PARAMS\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "        \n",
    "        # TEST\n",
    "        self.compute_prob(np.random.randn(obssize).reshape(1, -1))\n",
    "    \n",
    "    def compute_prob(self, states):\n",
    "        \"\"\"\n",
    "        compute prob distribution over all actions given state: pi(s)\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples, actsize]\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        prob = torch.nn.functional.softmax(self.model(states), dim=-1)\n",
    "        return prob.cpu().data.numpy()\n",
    "\n",
    "    def _to_one_hot(self, y, num_classes):\n",
    "        \"\"\"\n",
    "        convert an integer vector y into one-hot representation\n",
    "        \"\"\"\n",
    "        scatter_dim = len(y.size())\n",
    "        y_tensor = y.view(*y.size(), -1)\n",
    "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype)\n",
    "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
    "    \n",
    "    def train(self, states, actions, Qs):\n",
    "        \"\"\"\n",
    "        states: numpy array (states)\n",
    "        actions: numpy array (actions)\n",
    "        Qs: numpy array (Q values)\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        Qs = torch.FloatTensor(Qs)\n",
    "        \n",
    "        # COMPUTE probability vector pi(s) for all s in states\n",
    "        logits = self.model(states)\n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Compute probaility pi(s,a) for all s,a\n",
    "        action_onehot = self._to_one_hot(actions, actsize)\n",
    "        prob_selected = torch.sum(prob * action_onehot, axis=-1)\n",
    "        \n",
    "        # FOR ROBUSTNESS\n",
    "        prob_selected += 1e-8\n",
    "\n",
    "        # TODO define loss function as described in the text above\n",
    "        # loss = ....\n",
    "\n",
    "        # BACKWARD PASS\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.detach().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to rollout trajecories using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a set of template code for running a policy to interact with the environment\n",
    "# It initializes a policy and runs it\n",
    "# Note that you may not be able to run the code properly if there still some undefined components on the Policy class\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "policyinit=Policy(obssize, actsize, 0.1)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    prob = policyinit.compute_prob(np.expand_dims(obs,0))\n",
    "    prob /= np.sum(prob) #normalizing again to account for numerical errors\n",
    "    action = np.asscalar(np.random.choice(actsize, p=prob.flatten(), size=1)) #choose according distribution prob\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate $Q^\\pi(s,a)$\n",
    "\n",
    "To estimate $Q^\\pi(s,a)$, we can rollout the policy until the episode ends and do monte carlo estimate. In particular, under policy $\\pi$, we start from state action $(s_0,a_0)$ and rollout the policy to generate a trajectory $\\{s_0,a_0,s_1,a_1...s_T,a_T\\}$, with corresponding reward $r_0,r_1...r_T$. Monte carlo estimate is \n",
    "\n",
    "$$\\hat{Q}_{MC}(s,a) = \\sum_{t=0}^T r_t \\gamma^t \\approx Q^\\pi(s,a)$$\n",
    "\n",
    "This estimate by itself is of high variance. Using pure monte carlo estimate may work but the gradient can have large variance and hence take the algorithm  a long time to converge. We can reduce variance using baseline. Recall the derivation of PG\n",
    "\n",
    "$$\\nabla_\\phi \\rho(\\phi) = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty Q^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big] = \\mathbb{E}_{\\pi_\\phi} \\big[\\sum_{t=0}^\\infty ( Q^{\\pi_\\phi}(s_t,a_t) - b(s_t)) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
    "\n",
    "where $b(s_t)$ can be any function of state $s_t$. $b(s_t)$ is called baseline. Optimal baseline function is hard to compute, but a good proxy is the value function $V^{\\pi_\\phi}(s_t)$. Hence the gradient has the form \n",
    "$$\\nabla_\\phi \\rho(\\pi_{\\phi}) = \\mathbb{E}_{\\pi} \\big[\\sum_{t=0}^\\infty A^{\\pi_\\phi}(s_t,a_t) \\nabla_\\phi \\log \\pi_\\phi(s_t, a_t) \\big]$$\n",
    "\n",
    "where $A^{\\pi_\\phi}(s,a)$ is the advantage. Hence we can train a value function $V^{\\pi_\\phi}(s)$ along side the policy and use it as baseline to reduce the variance of PG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we also parameterize a value function $V_\\theta(s) \\approx V^{\\pi_\\phi}(s)$ with parameter $\\theta$ to serve as baseline. The function takes as input the states $s$ and outputs a real value. \n",
    "\n",
    "Notice that unlike DQN, where $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$, now we have $V_\\theta(s) \\approx V^{\\pi_\\phi}(s)$. Therefore, we have a moving target to approximate, that changes with the current policy $\\pi_\\phi$. As $\\phi$ is updated by PG, $\\pi_\\phi$ keeps changing, which changes $V^{\\pi_\\phi}(s)$ as well. We need to adapt $V_\\theta(s)$ online to cater for the change in policy. \n",
    "\n",
    "Recall that to evaluate a policy $\\pi$, we collect rollouts using $\\pi$. If we start with state $s_0$, the reward following $\\pi$ thereafter is $r_0...r_{T}$  then \n",
    "\n",
    "$$V^\\pi(s_0) \\approx \\sum_{t=0}^{T} r_t \\gamma^{t} = \\hat{V}(s_0)$$\n",
    "\n",
    "In general, given a trajectory $(s_0, a_0, s_1, a_1, r_1, s_2, a_2, r_2, ..., s_{T+1})$\n",
    "\n",
    "$$\\hat{V}(s_i) = \\sum_{i=t}^{T} r_i \\gamma^{i-t}$$\n",
    "\n",
    "And the objective to minimize over is \n",
    "$$\\frac{1}{T+1} \\sum_{i=0}^{T} (V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
    "\n",
    "Since the policy keeps updating, we do not need to minimize the above objective to optimality. In practice, taking one gradient step w.r.t. above objective suffices.\n",
    "\n",
    "In the code cell below, define the neural network to learn value function estimate. The implementation is similar to Qfunction class in lab3, except that inputs are only states, and not actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define value function as a class. You need to define the model and set the loss.\n",
    "\n",
    "class ValueFunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, lr):\n",
    "        \"\"\"\n",
    "        obssize: size of states\n",
    "        \"\"\"\n",
    "        # TODO DEFINE THE MODEL\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    #TODO\n",
    "                      #input layer of input size obssize\n",
    "                      #intermediate layers\n",
    "                      #output layer of output size 1\n",
    "                \n",
    "                )\n",
    "        \n",
    "        # DEFINE THE OPTIMIZER\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # RECORD HYPER-PARAMS\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "        \n",
    "        # TEST\n",
    "        self.compute_values(np.random.randn(obssize).reshape(1, -1))\n",
    "    \n",
    "    def compute_values(self, states):\n",
    "        \"\"\"\n",
    "        compute value function for given states\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples]\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        return self.model(states).cpu().data.numpy()\n",
    "    \n",
    "    def train(self, states, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array\n",
    "        targets: numpy array\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        # COMPUTE Value PREDICTIONS for states \n",
    "        v_preds = self.model(states)\n",
    "\n",
    "        # LOSS\n",
    "        # TODO: set LOSS as square error of predicted values compared to targets\n",
    "        #loss = \n",
    "\n",
    "        # BACKWARD PASS\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.detach().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of pseudocode\n",
    "\n",
    "The critical components of the pseudocode are as follows.\n",
    "\n",
    "**Collect trajectories** Given current policy $\\pi_\\phi$, we can rollout using the policy by executing $a_t \\sim \\pi_\\phi(s_t)$. \n",
    "\n",
    "**Update value function** Value function update is based on minimizing the L2 loss between predicted value function and estimated value functions. For each state $s_i, i=0,\\ldots, T$ in a trajectory of length $T+1$, compute $\\hat{V}(s_i)$ as dicounted reward over the rest of the path (as defined above).  \n",
    "Then take one gradient step to update $\\theta$ using the gradient of the following loss: \n",
    "\n",
    "$$\\frac{1}{T+1} \\sum_{i=0}^T(V_\\theta(s_i) - \\hat{V}(s_i))^2$$\n",
    "\n",
    "For your conveience, below we have provided a function discounted_rewards(r,gamma) that takes as inputs a list of $T$ rewards $r$ and computes all discounted rewards $\\hat V(s_i), i=0, 1, 2, \\ldots, T$. \n",
    "\n",
    "**Update policy using PG** To compute PG, we need to first monte carlo estimate action-value function $\\hat{Q}(s_i,a_i)$. Given a trajectory with rewards $r=[r_0, r_1, r_2, \\ldots, r_T]$, this can also be computed for all $s_i, a_i$ in this trajectory using the discounted_rewards(r, gamma) function below.\n",
    "\n",
    "Then use value function as a baseline to compute advantage\n",
    "\n",
    "$$\\hat{A}(s_i,a_i) = \\hat{Q}(s_i,a_i) - V_\\theta(s_i)$$\n",
    "\n",
    "Then compute surrogate loss \n",
    "\n",
    "$$L = - \\frac{1}{(T+1)}\\sum_{i} \\hat{A}(s_i,a_i) \\log \\pi(a_i|s_i) $$\n",
    "\n",
    "The policy is updated by $$\\phi \\leftarrow \\phi - \\alpha  \\nabla_\\phi L \\approx \\phi + \\alpha \\nabla_\\phi \\rho(\\pi_\\phi)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return list(discounted_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main implementation : Policy gradient algorithm\n",
    "\n",
    "Combine all the above steps and implement the policy gradient algorithm with value function baseline in the cell below. The use of baseline is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "#remove the above line if you do not want to see wandb plots in your notebook. You can always see them on the wandb website.\n",
    "\n",
    "#You can change the code in this cell anyway you want \n",
    "#However, just make sure per epsiode reward during the run of this algorithm is being recorded in list rrecord \n",
    "#and logged on wandb as in the last few lines.\n",
    "\n",
    "# parameter initializations (you can change any of these)\n",
    "alpha = 1e-2  # learning rate for PG\n",
    "beta = 1e-3  # learning rate for baseline\n",
    "numtrajs = 5  # num of trajecories from the current policy to collect in each iteration\n",
    "iterations = 300  # total num of iterations\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "gamma = .99  # discount\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# initialize networks\n",
    "actor = Policy(obssize, actsize, alpha)  # policy initialization: IMPORTANT: this is the policy you will be scored on\n",
    "baseline = ValueFunction(obssize, beta)  # baseline initialization\n",
    "\n",
    "#To record training reward for logging and plotting purposes\n",
    "rrecord = []\n",
    "\n",
    "# main iteration\n",
    "for ite in range(iterations): \n",
    "    \n",
    "    # To record traectories generated from current policy\n",
    "    OBS = []  # observations\n",
    "    ACTS = []  # actions\n",
    "    ADS = []  # advantages (to compute policy gradient)\n",
    "    VAL = []  # Monte carlo value predictions (to compute baseline, and policy gradient)\n",
    "\n",
    "    for num in range(numtrajs):\n",
    "        # To keep a record of states actions and reward for each episode\n",
    "        obss = []  # states\n",
    "        acts = []   # actions\n",
    "        rews = []  # instant rewards\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # TODO: run one episode using the current policy \"actor\" \n",
    "        # TODO: record all observations (states, actions, rewards) from the epsiode in  obss, acts, rews\n",
    "            \n",
    "        #Below is for logging training performance\n",
    "        rrecord.append(np.sum(rews))\n",
    "        \n",
    "        # TODO:  Use discounted_rewards function to compute \\hat{V}s/\\hat{Q}s  from instant rewards in rews\n",
    "        # TODO: record the computed \\hat{V}s in VAL, states obss in OBS, and actions acts in ACTS, for batch update\n",
    "    \n",
    "    # AFTER collecting numtrajs trajectories:\n",
    "    \n",
    "    #1. TODO: train baseline\n",
    "    \"\"\"\n",
    "        Use the batch (OBS, VAL) of states and value predictions as targets to train baseline. \n",
    "        Use baseline.train : note that this takes as input numpy array, so you may have to convert \n",
    "        lists into numpy array using np.array()\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2.TODO: Update the policy\n",
    "    \"\"\" \n",
    "        Compute baselines: use basline.compute_values for states in the batch OBS\n",
    "        Compute advantages ADS using VAL and computed baselines\n",
    "        Update policy using actor.train using OBS, ACTS and ADS\n",
    "    \"\"\"\n",
    "       \n",
    "     \n",
    "     \n",
    "    #printing moving averages for smoothed visualization. \n",
    "    #Do not change below: this assume you recorded the sum of rewards in each episide in the list rrecord\n",
    "    fixedWindow=100\n",
    "    movingAverage=0\n",
    "    if len(rrecord) >= fixedWindow:\n",
    "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "        \n",
    "    #wandb logging\n",
    "    wandb.log({ \"training reward\" : rrecord[-1], \"training reward moving average\" : movingAverage})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the trained policy. The evaluation will be run for 100 epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "\n",
    "### DO NOT CHANGE\n",
    "def evaluate(policy, env, episodes):\n",
    "    # main iteration\n",
    "    score = 0\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        rsum = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            p = policy.compute_prob(np.expand_dims(obs,0)).ravel()\n",
    "            p /= np.sum(p)\n",
    "            action = np.asscalar(np.random.choice(np.arange(2), size=1, p=p))\n",
    "\n",
    "            # env stepping forward\n",
    "            newobs, r, done, _ = env.step(action)\n",
    "\n",
    "            # update data\n",
    "            rsum += r\n",
    "            obs = newobs        \n",
    "\n",
    "        \n",
    "        wandb.log({\"eval reward\" : rsum})\n",
    "        score +=rsum\n",
    "    score = score/episodes\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the learned policy \"actor\"\n",
    "# on a target environment\n",
    "env_test = gym.make(envname)\n",
    "eval_episodes = 1000\n",
    "score = evaluate(actor, env_test, eval_episodes)\n",
    "wandb.run.summary[\"score\"]=score \n",
    "print(\"eval performance of the learned policy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
