{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"lab3\", entity=\"ieor-4575\", tags=[\"torch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In previous Labs, we have learned to use Pytorch to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter). \n",
    "\n",
    "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimality equation\n",
    "\n",
    "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
    "\n",
    "$$Q^\\ast(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_a Q^\\ast(s_{t+1},a)\\big]$$\n",
    "\n",
    "where the expectation is wrt the random reward $r_t$ and transition to the next state $s_{t+1}$. A natural objective to consider is \n",
    "\n",
    "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) -\\mathbb{E}\\big[r_t + \\gamma  \\max_a  Q_{\\hat \\theta}(s_{t+1},a)\\big])^2$$\n",
    "at the current or previous $\\hat \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the DQN model \n",
    "\n",
    "The first step is to build a neural network with parameters $\\theta$ that predicts $Q_\\theta(s,a)$ for any $(s,a)$. You can either build a network that \n",
    "\n",
    "* (in case of small number $K$ of discrete actions) takes as input a  representation of state $s$ and outputs a $K$-dimensional vector giving scores $Q(s,a), a=1,\\ldots, K$ for all actions\n",
    "\n",
    "or \n",
    "\n",
    "* takes as input a concatenated representation of state and action $(s,a)$ and output one dimensional score $Q_\\theta(s,a)$,\n",
    "\n",
    "Below we have provided a skeleton code (incomplete) for defining and training the Q-function. **You need to fill in the DNN model definition and loss function definition**. Refer to regression lab (lab 2) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, lr):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        sess: sess to execute this Qfunction\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        # DEFINE THE MODEL\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    #TODO      \n",
    "                    #input layer\n",
    "                    #torch.nn.Linear(<input size>, <intermediate layer size>),\n",
    "                    #torch.nn.ReLU(),\n",
    "                    #torch.nn.Linear(<intermediate layer size>, <intermediate layer size>),\n",
    "                    #torch.nn.ReLU(),\n",
    "                    #torch.nn.Linear(<intermediate layer size>, <output size>)\n",
    "                )\n",
    "        \n",
    "        # DEFINE THE OPTIMIZER\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # RECORD HYPER-PARAMS\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "        \n",
    "    def _to_one_hot(self, y, num_classes):\n",
    "        \"\"\"\n",
    "        convert an integer vector y into one-hot representation\n",
    "        \"\"\"\n",
    "        scatter_dim = len(y.size())\n",
    "        y_tensor = y.view(*y.size(), -1)\n",
    "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype)\n",
    "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states, actions):\n",
    "        \"\"\"\n",
    "        input: list of numsamples state-action pairs\n",
    "        output: List of Q values for each input (s,a). The output will have size [numsamples, 1] \n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #This will be different for neural network that takes as input a state-action pair\n",
    "        \n",
    "        #states = torch.FloatTensor(states)\n",
    "        #q_preds = self.model(states)\n",
    "        #action_onehot = self._to_one_hot(actions, actsize)\n",
    "        #q_preds_selected = torch.sum(q_preds * action_onehot, axis=-1)\n",
    "\n",
    "        return q_preds_selected\n",
    "        \n",
    "    def compute_maxQvalues(self, states):\n",
    "        \"\"\"\n",
    "        input: a list of numsamples states \n",
    "        output: max_a Q(s,a) values for every input state s in states. The output will have size numsamples\n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
    "        \n",
    "        #states = torch.FloatTensor(states)\n",
    "        #Qvalues = self.model(states).cpu().data.numpy()\n",
    "        #q_preds_greedy = np.max(Qvalues,1) \n",
    "        \n",
    "        return q_preds_greedy\n",
    "        \n",
    "    def compute_argmaxQ(self, state):\n",
    "        \"\"\"\n",
    "        input: one state s\n",
    "        output: arg max_a Q(self.model(states).cpu().data.numpy()s,a) values for the input state s. The output will have size 1\n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
    "        \n",
    "        #state = torch.FloatTensor(state)\n",
    "        #Qvalue = self.model(state).cpu().data.numpy()\n",
    "        #greedy_action = np.argmax(Qvalue.flatten())\n",
    "        \n",
    "        return greedy_action\n",
    "        \n",
    "        \n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        # COMPUTE Q PREDICTIONS for all state-action pairs\n",
    "        q_preds_selected = self.compute_Qvalues(states, actions)\n",
    "\n",
    "                \n",
    "        # LOSS\n",
    "        #print(q_preds_selected.shape, targets.shape)\n",
    "        loss = torch.mean((q_preds_selected - targets)**2)\n",
    "\n",
    "        # BACKWARD PASS\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.detach().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can skip ahead to implementing the basic Q-learning that at every step $t$ in the environment\n",
    "* given state $s_t$, computes greedy actions from Q-values (using compute_argmaxQ function above) and uses $\\epsilon$-greedy select an action $a_t$, \n",
    "* makes observation of reward $r_t$ and next state $s_{t+1}$\n",
    "* using compute_maxQvalues() function, computes target\n",
    "  $$r_t + \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
    "and then retrains the Q-function using train() function above (with numsamples=1)\n",
    "\n",
    "However, for improved performance you may want to consider ideas like batch training (numsamples>1 is the batch size) with experience replay buffer and target-networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Buffer**\n",
    "\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_i,a_i,r_i,s_{i}') \\sim R$ and then compute\n",
    "targets\n",
    "\n",
    "$$d_i=r_i + \\max_a \\gamma Q_{\\theta}(s_i^\\prime,a)$$\n",
    "for all $i$. Use the above training function train() with input as list $(s_i, a_i, d_i)_{i=1}^N$  to update parameters using backprop.\n",
    "\n",
    "**Target Network**\n",
    "\n",
    "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^-$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i =  r_t + \\gamma \\max_a Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
    "\n",
    "**Initialization.**\n",
    "principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty). \n",
    "\n",
    "**At each time step $t.$**\n",
    "The agent executes action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - (r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
    "\n",
    "**Update target network.**\n",
    "Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$.\n",
    "\n",
    "**Bellman target.**\n",
    "Above, we have defined the target values as being computed from a target net with parameter $\\theta^-$ \n",
    "$$r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a)$$\n",
    "It is worth thinking about what happens if we are at the end of an episode, that is, what if $s_i^\\prime$ here is a terminal state. In this case, should the Bellman error be defined exactly the same as above? Do we need some modifications? Think carefully about this as this will greatly impact the algorithmic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.buffer,batchsize)\n",
    "        return minibatch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippet for copying target network\n",
    "You may use th following to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $\\theta^- \\leftarrow \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_target_update(Qprincipal, Qtarget):\n",
    "    for v,v_ in zip(Qprincipal.model.parameters(), Qtarget.model.parameters()):\n",
    "        v_.data.copy_(v.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for DQN \n",
    "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "#remove above line if you do not want to see inline plots from wandb\n",
    "\n",
    "# hyper-parameters\n",
    "lr = 1e-3  # learning rate for gradient update\n",
    "batchsize = 64  # batchsize for buffer sampling\n",
    "maxlength = 1000  # max number of tuples held by buffer\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "tau = 100  # time steps for target update\n",
    "episodes = 300  # number of episodes to run\n",
    "initialsize = 500  # initial time steps before start training\n",
    "epsilon = .2  # constant for exploration\n",
    "gamma = .99  # discount\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# initialize Q-function networks (princpal and target)\n",
    "Qprincipal = Qfunction(obssize, actsize, lr)\n",
    "Qtarget = Qfunction(obssize, actsize, lr)\n",
    "\n",
    "# initialization of graph and buffer\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "\n",
    "# main iteration\n",
    "rrecord = []\n",
    "totalstep = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        #greedy choice below. Use epsilon greedy for exploration\n",
    "        action = Qprincipal.compute_argmaxQ(np.expand_dims(obs,0))\n",
    "        \n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        done_ = 1 if done else 0\n",
    "        e = (obs, action, r, done_, newobs)\n",
    "\n",
    "        #IF NOT USING BUFFER:\n",
    "        #use single sample (obs, action, r, done_, newobs) with Qtarget to compute target and train Qprincipal\n",
    "        \n",
    "        # ELSE IF USING REPLAY BUFFER\n",
    "        # append experiences e to buffer\n",
    "        \"\"\"\n",
    "        buffer.append(e)\n",
    "        while buffer.number > maxlength:\n",
    "            buffer.pop()\n",
    "        \"\"\"\n",
    "        #every few episodes (decide the frequency) sample a minibatch from buffer\n",
    "        #compute targets in batch using Qtarget and train  Qprincipal \n",
    "        \n",
    "        \n",
    "        #UPDATE target network \n",
    "        #every tau steps update copy the principal network to the target network\n",
    "        \"\"\"\n",
    "        if totalstep % tau == 0:\n",
    "            run_target_update(Qprincipal, Qtarget)\n",
    "        \"\"\"\n",
    "        \n",
    "        # update\n",
    "        totalstep += 1\n",
    "        rsum += r\n",
    "        obs = newobs \n",
    "    \n",
    "    #The code below is for printing and debugging at the end of episode\n",
    "    \n",
    "    rrecord.append(rsum)\n",
    "    \n",
    "    # printing functions for debugging purposes. Feel free to add more\n",
    "    #if episode % 10 == 0:\n",
    "     #   print('buffersize {}'.format(buffer.number))\n",
    "     #   print('episode {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
    "    \n",
    "    #printing moving averages for smoothed visualization. \n",
    "    fixedWindow=100\n",
    "    movingAverage=0\n",
    "    if len(rrecord) >= fixedWindow:\n",
    "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "    wandb.log({ \"training reward\" : rsum, \"train reward moving average\" : movingAverage})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the greedy policy wrt learned Q-function. The evaluation will be run 10 times, each for eval_epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE\n",
    "def evaluate(Q, env, episodes):\n",
    "    # main iteration\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        rsum = 0\n",
    "        \n",
    "        while not done:\n",
    "            # always greedy\n",
    "            action = Q.compute_argmaxQ(np.expand_dims(obs,0))\n",
    "            \n",
    "\n",
    "            # mdp stepping forward\n",
    "            newobs, r, done, _ = env.step(action)\n",
    "\n",
    "            # update data\n",
    "            rsum += r\n",
    "            obs = newobs        \n",
    "\n",
    "        \n",
    "        wandb.log({\"eval reward\" : rsum})\n",
    "\n",
    "    return rsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "env_test = gym.make(envname)\n",
    "eval_episodes = 300\n",
    "score = np.mean([evaluate(Qprincipal, env_test, eval_episodes) for k in range(10)])\n",
    "wandb.run.summary[\"score\"]=score \n",
    "\n",
    "print(\"eval performance of DQN agent: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
